{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Data Processing and Feature Engineering\n",
    "\n",
    "## Objective\n",
    "In this notebook, we transform our raw data into a clean, unified dataset ready for analysis. Our goal is to calculate a **\"Demand Score\"** for each neighborhood.\n",
    "\n",
    "## Steps\n",
    "1.  **Load Data**: Reload our raw datasets (Chargers, Vehicles, Income, Neighborhoods).\n",
    "2.  **Clean & Standardize**: Ensure all datasets use compatible naming conventions (e.g., Neighborhood Codes).\n",
    "3.  **Spatial Join**: Determine which neighborhood each existing charger belongs to.\n",
    "4.  **Aggregate Data**: Calculate total chargers, average income, and vehicle metrics per neighborhood.\n",
    "5.  **Calculate Demand Score**: Create a unified metric to identify high-potential areas.\n",
    "6.  **Export**: Save the processed data for the next phase (Optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Dynamic Path Setup (so this runs on any machine)\n",
    "def get_data_dir():\n",
    "    # Check common paths relative to the notebook\n",
    "    possible_paths = ['../data/raw', 'data/raw', '../../data/raw']\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    # Fallback to absolute path logic if needed\n",
    "    return os.path.join(os.getcwd(), 'data', 'raw')\n",
    "\n",
    "RAW_DATA_DIR = get_data_dir()\n",
    "print(f\"Loading data from: {os.path.abspath(RAW_DATA_DIR)}\")\n",
    "\n",
    "FILE_GEOMETRY = os.path.join(RAW_DATA_DIR, '0301100100_UNITATS_ADM_POLIGONS.json')\n",
    "FILE_CHARGERS = os.path.join(RAW_DATA_DIR, '2023_2T_Punts_Recarrega_Vehicle_Electric.json')\n",
    "FILE_VEHICLES = os.path.join(RAW_DATA_DIR, '2024_parc_vehicles_tipus_propulsio.csv')\n",
    "FILE_INCOME = os.path.join(RAW_DATA_DIR, '2022_renda_disponible_llars_per_persona.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We use the robust loading logic we developed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Neighborhoods (Geometry)\n",
    "gdf_barris = gpd.read_file(FILE_GEOMETRY)\n",
    "# Ensure CRS is standard WGS84 (Latitude/Longitude)\n",
    "if gdf_barris.crs != 'EPSG:4326':\n",
    "    gdf_barris = gdf_barris.to_crs('EPSG:4326')\n",
    "\n",
    "print(f\"Loaded {len(gdf_barris)} administrative units.\")\n",
    "\n",
    "# Load Chargers (JSON -> GeoDataFrame)\n",
    "try:\n",
    "    gdf_chargers = gpd.read_file(FILE_CHARGERS)\n",
    "except:\n",
    "    # Fallback for flat JSON\n",
    "    df_temp = pd.read_json(FILE_CHARGERS)\n",
    "    gdf_chargers = gpd.GeoDataFrame(\n",
    "        df_temp,\n",
    "        geometry=gpd.points_from_xy(df_temp.Station_lng, df_temp.Station_lat),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "print(f\"Loaded {len(gdf_chargers)} chargers.\")\n",
    "\n",
    "# Load Income\n",
    "df_income = pd.read_csv(FILE_INCOME)\n",
    "print(f\"Loaded income data: {len(df_income)} rows.\")\n",
    "\n",
    "# Load Vehicles\n",
    "df_vehicles = pd.read_csv(FILE_VEHICLES)\n",
    "print(f\"Loaded vehicle data: {len(df_vehicles)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Standardization\n",
    "We need to filter the geometry to only keep Neighborhoods ('BARRI') and then standardize the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect column names to ensure we pick the right ones\n",
    "# print(\"Geometry Columns:\", gdf_barris.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize 'Codi_Barri' to Integer\n",
    "\n",
    "# Neighborhoods (Geometry)\n",
    "# The file contains multiple administrative levels (District, Neighborhood, etc.)\n",
    "# We need to filter only for \"BARRI\"\n",
    "if 'TIPUS_UA' in gdf_barris.columns:\n",
    "    print(f\"Filtering geometry for 'BARRI' levels. Original size: {len(gdf_barris)}\")\n",
    "    gdf_barris = gdf_barris[gdf_barris['TIPUS_UA'] == 'BARRI'].copy()\n",
    "    print(f\"Filtered size: {len(gdf_barris)}\")\n",
    "\n",
    "# Identify the correct column for Neighborhood Code\n",
    "col_barri_geo = 'BARRI'\n",
    "if col_barri_geo not in gdf_barris.columns:\n",
    "    candidates = [c for c in gdf_barris.columns if 'BARRI' in c.upper()]\n",
    "    col_barri_geo = candidates[0] if candidates else 'CODI_UA'\n",
    "\n",
    "print(f\"Using '{col_barri_geo}' as Geometry Neighborhood Code column.\")\n",
    "\n",
    "# Clean the column (handle \"1\" vs 1 vs \"01\")\n",
    "gdf_barris['Barri_ID'] = gdf_barris[col_barri_geo].astype(str).str.strip().astype(int)\n",
    "\n",
    "# Income\n",
    "df_income['Barri_ID'] = df_income['Codi_Barri'].astype(int)\n",
    "\n",
    "# Vehicles\n",
    "# 'Codi_Barri' may contain non-numeric values like 'NC' (No Consta)\n",
    "# We convert them to NaN, then drop those rows, then convert to int.\n",
    "df_vehicles['Barri_ID'] = pd.to_numeric(df_vehicles['Codi_Barri'], errors='coerce')\n",
    "print(f\"Dropping {df_vehicles['Barri_ID'].isna().sum()} rows with invalid Barri ID in vehicles data.\")\n",
    "df_vehicles = df_vehicles.dropna(subset=['Barri_ID'])\n",
    "df_vehicles['Barri_ID'] = df_vehicles['Barri_ID'].astype(int)\n",
    "\n",
    "print(\"Standardization complete. Using 'Barri_ID' as the common key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregating Socio-Economic Data\n",
    "We need one row per Neighborhood.\n",
    "\n",
    "### 4.1 Income\n",
    "We'll take the mean income per neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_agg = df_income.groupby('Barri_ID')['Import_Euros'].mean().reset_index()\n",
    "df_income_agg.rename(columns={'Import_Euros': 'Avg_Income'}, inplace=True)\n",
    "display(df_income_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Vehicles\n",
    "We now have a file broken down by propulsion type. We will calculate:\n",
    "1. **Total Vehicles**: General traffic volume.\n",
    "2. **EV Count**: Count of 'Elèctrica' and 'Híbrid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Total Vehicles AND EV Counts by Neighborhood\n",
    "if 'Nombre' in df_vehicles.columns:\n",
    "    print(\"Found 'Nombre' column. Calculating Total Vehicles and EVs...\")\n",
    "    \n",
    "    # 1. Total Vehicles\n",
    "    df_vehicles_agg = df_vehicles.groupby('Barri_ID')['Nombre'].sum().reset_index()\n",
    "    df_vehicles_agg.rename(columns={'Nombre': 'Total_Vehicles'}, inplace=True)\n",
    "    \n",
    "    # 2. Electric Vehicles (Elèctrica + Híbrid)\n",
    "    # We filter for 'Elèctrica' and 'Híbrid'\n",
    "    target_propulsion = ['Elèctrica', 'Híbrid']\n",
    "    df_evs = df_vehicles[df_vehicles['Tipus_Propulsio'].isin(target_propulsion)]\n",
    "    \n",
    "    df_ev_agg = df_evs.groupby('Barri_ID')['Nombre'].sum().reset_index()\n",
    "    df_ev_agg.rename(columns={'Nombre': 'EV_Count'}, inplace=True)\n",
    "    \n",
    "    # Merge EV count into main aggregate\n",
    "    df_vehicles_agg = df_vehicles_agg.merge(df_ev_agg, on='Barri_ID', how='left')\n",
    "    df_vehicles_agg['EV_Count'] = df_vehicles_agg['EV_Count'].fillna(0)\n",
    "    \n",
    "else:\n",
    "    print(\"Error: 'Nombre' column not found in vehicle dataset!\")\n",
    "    # Fallback\n",
    "    df_vehicles_agg = df_vehicles.groupby('Barri_ID').size().reset_index(name='Total_Vehicles')\n",
    "    df_vehicles_agg['EV_Count'] = 0\n",
    "\n",
    "display(df_vehicles_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spatial Join: Mapping Chargers to Neighborhoods\n",
    "We have charger points (Lat/Lon). We need to count how many chargers are inside each Neighborhood polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Spatial Join (Left Join: Chargers -> Neighborhoods)\n",
    "# We strictly use 'intersects' or 'within'\n",
    "gdf_joined = gpd.sjoin(gdf_chargers, gdf_barris, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Count chargers per Barri\n",
    "chargers_per_barri = gdf_joined.groupby('Barri_ID').size().reset_index(name='Charger_Count')\n",
    "display(chargers_per_barri.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating the Master Dataset (Barrios with Demand)\n",
    "Now we merge everything into the master `gdf_barris` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start with Neighborhood Geometry\n",
    "master_df = gdf_barris[['Barri_ID', 'NOM', 'geometry']].copy() # Selecting key columns\n",
    "\n",
    "# 2. Merge Income\n",
    "master_df = master_df.merge(df_income_agg, on='Barri_ID', how='left')\n",
    "\n",
    "# 3. Merge Vehicles\n",
    "master_df = master_df.merge(df_vehicles_agg, on='Barri_ID', how='left')\n",
    "\n",
    "# 4. Merge Charger Counts\n",
    "master_df = master_df.merge(chargers_per_barri, on='Barri_ID', how='left')\n",
    "\n",
    "# Fill NaNs (if a barrio has no chargers, count is 0)\n",
    "master_df['Charger_Count'] = master_df['Charger_Count'].fillna(0)\n",
    "# Fill missing income/vehicles with median (to avoid breaking the model)\n",
    "master_df['Avg_Income'] = master_df['Avg_Income'].fillna(master_df['Avg_Income'].median())\n",
    "master_df['Total_Vehicles'] = master_df['Total_Vehicles'].fillna(master_df['Total_Vehicles'].median())\n",
    "master_df['EV_Count'] = master_df['EV_Count'].fillna(0)\n",
    "\n",
    "display(master_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate \"Demand Score\"\n",
    "We create a composite score.\n",
    "\n",
    "**New Logic (incorporating EVs):**\n",
    "*   **EV Count** (50%): Strongest indicator. Areas with EVs *need* chargers right now.\n",
    "*   **Family Income** (30%): Indicator of purchasing power for future EVs.\n",
    "*   **Total Vehicles** (20%): Indicator of general traffic density and potential market size.\n",
    "\n",
    "$$ Score = (Norm\\_EVs \\times 0.5) + (Norm\\_Income \\times 0.3) + (Norm\\_TotalVehicles \\times 0.2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize Features\n",
    "master_df['Norm_Income'] = scaler.fit_transform(master_df[['Avg_Income']])\n",
    "master_df['Norm_Vehicles'] = scaler.fit_transform(master_df[['Total_Vehicles']])\n",
    "master_df['Norm_EVs'] = scaler.fit_transform(master_df[['EV_Count']])\n",
    "\n",
    "# Calculate Demand Score (0 to 100)\n",
    "# Weight: 50% Existing EVs, 30% Income, 20% Total Traffic\n",
    "master_df['Demand_Score'] = (\n",
    "    master_df['Norm_EVs'] * 0.5 + \n",
    "    master_df['Norm_Income'] * 0.3 + \n",
    "    master_df['Norm_Vehicles'] * 0.2\n",
    ") * 100\n",
    "\n",
    "# Display top 5 neighborhoods by Demand\n",
    "display(master_df[['NOM', 'Demand_Score', 'EV_Count', 'Total_Vehicles', 'Avg_Income']].sort_values(by='Demand_Score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data\n",
    "We save this master dataset to `data/processed` for the optimization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = '../data/processed'\n",
    "if not os.path.exists(PROCESSED_DIR):\n",
    "    # Try absolute if relative fails (for robustness)\n",
    "    PROCESSED_DIR = os.path.join(os.getcwd(), '..', 'data', 'processed')\n",
    "    if not os.path.exists(PROCESSED_DIR):\n",
    "         try:\n",
    "             os.makedirs(PROCESSED_DIR)\n",
    "         except:\n",
    "             # last resort basic\n",
    "             PROCESSED_DIR = 'data/processed'\n",
    "             os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "OUT_FILE = os.path.join(PROCESSED_DIR, 'barrios_with_demand.geojson')\n",
    "\n",
    "# Save as GeoJSON (preserves geometry)\n",
    "master_df.to_file(OUT_FILE, driver='GeoJSON')\n",
    "\n",
    "print(f\"Success! Data saved to: {os.path.abspath(OUT_FILE)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}